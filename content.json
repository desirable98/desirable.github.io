{"pages":[{"title":"Welcome to Xinru Jia&#39;s Personal Homepage!","text":"About me Education &amp; Experience Reaserch Direction Honors Social Activities My Posts Categories About meI am Xinru Jia(贾心茹), Currently I am a Master student at Fudan University, advised by Qi Liu in the Frontier Institute of Chip and System &amp; Microelectronics, Fudan Univesity. I am also working closely with Chixiao Chen. Education &amp; Experience 2013.09 - 2016.06, No.1 Middle School, Taian, Shandong 2019.09 - 2020.06, Dalian University of Technology(DUT), IC Design and Integrated System 2020.09 ~ , Fudan Univesity, IC and System Design Reaserch Direction Key word Spotting Artificial Intelligence Processor Computing in Memory Honors 2016 - 2017 National Scholarship 2017 Merit Student of Dalian 2017 - 2018 National Scholarship 2017 Provincial first prize in tiche 9th National Mathematics Competition for College Students 2018 - 2019 National Scholarship 2020 Outstanding graduate of LiaoNing Province Social Activities Youth League Secretary of Micro Electronic in DUT Office Minister of Student Union in DUT My PostsCategories KWS Computer and Intelligent Processor Architecture","link":"/Home/index.html"},{"title":"Welcome to Xinru Jia&#39;s Personal Homepage!","text":"About me Education &amp; Experience Reaserch Direction Honors Social Activities About meI am Xinru Jia(贾心茹), Currently I am a Master student at Fudan University, advised by Qi Liu in the Frontier Institute of Chip and System &amp; Microelectronics, Fudan Univesity. I am also working closely with Chixiao Chen. Education &amp; Experience 2013.09 - 2016.06, No.1 Middle School, Taian, Shandong 2019.09 - 2020.06, Dalian University of Technology(DUT), IC Design and Integrated System 2020.09 ~ , Fudan Univesity, IC and System Design Reaserch Direction Key word Spotting Artificial Intelligence Processor Computing in Memory Honors 2016 - 2017 National Scholarship 2017 Merit Student of Dalian 2017 - 2018 National Scholarship 2017 Provincial first prize in tiche 9th National Mathematics Competition for College Students 2018 - 2019 National Scholarship 2020 Outstanding graduate of LiaoNing Province Social Activities Youth League Secretary of Micro Electronic in DUT Office Minister of Student Union in DUT","link":"/about/index.html"}],"posts":[{"title":"如何设计一个AI加速器","text":"MFCC Hamming Window FFT log Cordic Neural Network MFCCHamming WindowFFTlogCordicNeural Network等我什么时候心情好就来更titi xiong $x=122$ 123import pynqol = pynq.Overlay(mybit, download=True) 122 123 123 123","link":"/2021/04/24/fft-accelerator/"},{"title":"CA-HW-1","text":"HW_1The assignment of HW_1.I will adjust the format and combine problem and solution to be more suitable for markdown, but you can get and edit it in Overleaf by the source file.And you can get my source code solution and commit file in github repo Problem 1: Implement a simple RISC Core (2+5+8=15 Points) On class, we define an extremely simple RISC ISA and its hardware organization, which are both shown below. For this homework assignment, we need to design/implement/simulate this core. Problem(a)Which HDL you want to use ? Solution(a)System Verilog Problem(b)For Verilog-players, please write an top-level structural verilog file to decribe the system. For C-players, please complete a header filer and .cc file , compatible to Vivado HLS, to describe the system. (Hint: Please submit your script as well) Solution(b)In the attachment I submitted, The tb-top.v is my testbench, and top.v is the top file of the whole core mudule, including 5 subfiles for circuit sub-module. You can run testbench.v file to get the simulated waveform. I also submitted the waveform file, whose screencut at the bottom of the document. Problem(c)For Verilog-players, please write a testbench verilog file to simulate the system. It should complete a 4-MAC (neuron) computing. For C-players, please complete a test.cc file , compatible to Vivado HLS, to simulate the system. It should complete a 4-MAC (neuron) computing. (Hint: Please submit your script and simulated waveform, highlight the final results and compare it with the theoretical value.) Solution(c)$$ \\left[ \\begin{matrix} -1 &amp; 2 &amp; -3 &amp; 4 \\ \\end{matrix} \\right] * \\left[ \\begin{matrix} -1 &amp; -1 &amp; 0 &amp; 2 \\\\ 2 &amp; 2 &amp; 3 &amp; 1 \\\\ 3 &amp; -3 &amp; 1 &amp; 4 \\\\ 4 &amp; 4 &amp; 2 &amp; 5 \\end{matrix} \\right] = \\left[ \\begin{matrix} 10 &amp; 30 &amp; 11 &amp; 8 \\ \\end{matrix} \\right]$$ In the testbench called top_tb.v, I designed a 4-mac Matrix to achieve. At first I stored the calculating data(int16) in dmem, and the instructions in imem at the at the initial of simulation. And part of addresses were stored in register file.In each cycle, I loaded the data from dmem and calculated them. We also need an extra instruction to reset the psum register. At the end of cycle, the sum was restored in dmem, and the address was just we prestore in the rf(rs in assemble codes). So we need such clk/instructions below assemble codes to complete the whole cycle. Lack for branch instruction, so we have to repeat similar instructions to calculate 4 cycles. 1234567891011121314load rd1, rs, 1load rd2, rs, 2load rd3, rs, 3load rd4, rs, 4load rd5, rs, 5load rd6, rs, 6load rd7, rs, 7load rd8, rs, 8mac rd , 0 , /, 1mac rd , rd1, rd2,0mac rd , rd3, rd4,0mac rd , rd5, rd6,0mac rd , rd7, rd8,0store / , rs , rd As showed in picture waveform, testdata was stored in Dmem at the beginning , after 4 cycles calculation, dmem was put in 10, 30, 11, 8 one by one (Display in hexadecimal). But We have reached the expected functional goal. ******Believe me, Support me, Pay me!****** ⬇︎","link":"/2021/04/30/CA-HW-1/"},{"title":"Computer and Intelligent Processor Architecture","text":"写在开头:&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;本学期我将该课程作为培养方案中的跨一级选修，在个人博客上同步更新每次作业思路，源代码及课程提交报告详见我的github repo，希望能给学习该课程的同学提供一点帮助，也希望各位同学批评指正。 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;此课程为复旦大学工研院陈迟晓老师开设的计算机与智能处理器体系结构线上课程。你可以在elearning课程主页上找到关于该课程的所有课程视频，课程ppt，三次小作业和最终project。同时有未知同学将该课程搬运到bilibili，但不属于老师本人维护，请大家尽可能在课程主页上学习，方便及时掌握更新内容。&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;同时提供几门课程及书籍作为该课程的前置参考资料： csapp 必须承认，作为硬件学习者csapp yyds，每个作业都非常经典且有趣，而且是非常基础的课程（虽然懒惰的我并没有全做完，但是作业答案各种开源代码还蛮多的，等我有空也可以在github开源一份，但可能懒得写blog 深入理解计算机系统 这本书的原作者就是csapp的主讲老师，csapp也是基于该书进行课程讲解，如果没时间看视频可以配合学习 计算机体系结构:量化研究方法 这本书需要在上述基础上进行学习，目前更新到第六版，但是第六版说翻译了两年了也没见中文版出现，超链接为浙大课程共享中的第六版英文版，同时同一个repo下也有其他几版教材中英版本 人工智能芯片设计 这本书由清华大学尹首一撰写，比较适合神经网络的入门（当然也适合写本子，逃～ 如果你是复旦大学微电子学院本科生，同样欢迎选修严昌浩老师（or 曾璇）教授的计算机软件基础，和韩军老师教授的专用集成电路系统专题讲座(名义讲座，实则硬核，但是有个助教小哥哥很帅，另一个助教小哥哥超强 待补充~","link":"/2021/04/30/Computer-and-Intelligent-Processor-Architecture/"},{"title":"CA-HW-3","text":"","link":"/2021/04/30/CA-HW-3/"},{"title":"CA-HW-2","text":"HW_2The assignment of HW_2.I will adjust the format and combine problem and solution to be more suitable for markdown, but you can get and edit it in Overleaf by the source file.And you can get my source code solution and commit file in github repo Problem 1: Implement a simple RISC Core (2+5+8=15 Points) Using the following ISA and hardware architecture to compute $\\mathbf{A} \\cdot \\mathbf{B} + \\mathbf{C}$, where$\\mathbf{A}$, $\\mathbf{B}$ and $\\mathbf{C}$ are $8\\times 8$ matrices.Each element in them are signed integers with 8b length. Problem(a) Write the entire assembly code for computation. (hints: 8 indexed vector register file is not sufficient for 8x8 matrix.) Problem(b) Propose a superscalar strategy (maximum 2 instruction per fetch), and calculate how many cycles needed. Compare the utilization ratio with and without the superscalar strategy. SolutionProblem(a) This problem is essentially a matrix block problem, which is very common in the data mapping process of neural network accelerators——The memory on chip is not sufficient, so we have to divide the activation into many tile and calculate each tile orderly.Depending on the split ways, I provide two methods to deal with the problem a. Solution(1)At first, we split three $8\\times8$ matrices A B C into 48 vectors which has 4 implements. Because it can match the DCM vector and Vector reg file bitwidth (32b).Matrix A is divided horizontally, and matrix B is divided vertically, just as the order of calculation.And we distinguish them by H and L,which is also easily for us to annotate assembly code.Each input vectors are stored in DCM vector. So the $8\\times8$ matrices calculation can be display as follow(The order of matrices C and S depends on the split methods of matrix B): $$ \\left[ \\begin{matrix} A1H &amp; A1L \\\\ A2H &amp; A2L \\\\ A3H &amp; A3L \\\\ A4H &amp; A4L \\\\ A5H &amp; A5L \\\\ A6H &amp; A6L \\\\ A7H &amp; A7L \\\\ A8H &amp; A8L \\end{matrix} \\right] * \\left[ \\begin{matrix} B1H &amp; B2H &amp; B3H &amp; B4H &amp; B5H &amp; B6H &amp; B7H &amp; B8H\\\\ B1L &amp; B2L &amp; B3L &amp; B4L &amp; B5L &amp; B6L &amp; B7L &amp; B8L \\end{matrix} \\right] \\\\ + \\left[ \\begin{matrix} C1H &amp; C1L \\\\ C2H &amp; C2L \\\\ C3H &amp; C3L \\\\ C4H &amp; C4L \\\\ C5H &amp; C5L \\\\ C6H &amp; C6L \\\\ C7H &amp; C7L \\\\ C8H &amp; C8L \\end{matrix} \\right] = \\left[ \\begin{matrix} S1H &amp; S1L \\\\ S2H &amp; S2L \\\\ S3H &amp; S3L \\\\ S4H &amp; S4L \\\\ S5H &amp; S5L \\\\ S6H &amp; S6L \\\\ S7H &amp; S7L \\\\ S8H &amp; S8L \\end{matrix} \\right]$$In instructions, vrd and vrs only have 3bit which is not sufficient for 48 vectors indexing in vector reg file. But we know that, the vectors of same type are stored in contiguous address memory. So we store the base address of input vectors -activation A, weight B and bias C in Scalar reg file, and mark all of them as RF1, RF2, RF3 easily. The same, we need restore the sum vector in DCM vector, and the base address is stored in Scalar reg file called RF4. In this way, we can use the Vload and Vstore instructions and index vectors by stored base address RF1,RF2 and Immediate.However, we use weight stationary to get over memory Wall, 8 indexed vector reg file is not sufficient. We cannot store all the weight in the DCM(8),so I split the weight matrix B into 4 part furthermore like this: $$\\left[ \\begin{matrix} B1H &amp; B2H &amp; B3H &amp; B4H \\end{matrix} \\right]\\$$$$\\left[ \\begin{matrix} B5H &amp; B6H &amp; B7H &amp; B8H \\end{matrix}\\right]$$$$\\left[ \\begin{matrix} B1L &amp; B2L &amp; B3L &amp; B4L \\end{matrix} \\right]\\$$$$\\left[ \\begin{matrix} B5L &amp; B6L &amp; B7L &amp; B8L \\end{matrix}\\right]$$ Besides, we calculate matrix S twice depended on the partitioned matrix A.In a word,my design assembly code looks like this : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126// Step1:load the weight// Matric B split1Vload VRF1, RF2, $0 //load B1HVload VRF2, RF2, $1 //load B2HVload VRF3, RF2, $2 //load B3HVload VRF4, RF2, $3 //load B4H// Step2: Initial BiasVload VRF5, RF3, $0 //load C1HVMAC VRF8, VRF5, /, $1000 //Add C1H// Step3: ComputingVload VRF5, RF1, $0 //load A1HVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //VMAC \\ , VRF3, VRF5, $0010 //VMAC VRF8, VRF4, VRF5, $0111 //Partial S1H=A1H * BiH + C1HVStore $0 , RF4, VRF8 //store partial S1H//Then we cycle above 2 steps 8 times till A8H•••// Step2: Initial BiasVload VRF5, RF3, $7 //C8HVMAC VRF8, VRF5, /, $1000 //// Step3: ComputingVload VRF5, RF1, $7 //A8HVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //VMAC \\ , VRF3, VRF5, $0010 //VMAC VRF8, VRF4, VRF5, $0111 //Partial S8HVStore $7 , RF4, VRF8// Step1:exchange the weight B, Bias C and Psum SVload VRF1, RF2, $4 //B4HVload VRF2, RF2, $5 //B5HVload VRF3, RF2, $6 //B6HVload VRF4, RF2, $7 //B7H // Step2: Initial BiasVload VRF5, RF3, $8 //C1LVMAC VRF8, VRF5, /, $1000 //// Step3: ComputingVload VRF5, RF1, $0 //A1HVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //VMAC \\ , VRF3, VRF5, $0010 //VMAC VRF8, VRF4, VRF5, $0111 //Partial S1LVStore $8 , RF4, VRF8//Then we cycle above 2 steps 8 times till A8H•••// Step2: Initial BiasVload VRF5, RF3, $15 //C8LVMAC VRF8, VRF5, /, $1000 //// Step3: ComputingVload VRF5, RF1, $7 //A8HVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //VMAC \\ , VRF3, VRF5, $0010 //VMAC VRF8, VRF4, VRF5, $0111 //Partial S8LVStore $15 , RF4, VRF8// Step1:exchange the weight B and Psum SVload VRF1, RF2, $8 //B1LVload VRF2, RF2, $9 //B2LVload VRF3, RF2, $10 //B3LVload VRF4, RF2, $11 //B4L// Step4: Reload PsumVload VRF5, RF4, $0 //load Partial S1H = A1H * BiH + C1HVMAC VRF8, VRF5, /, $1000 //// Step3: ComputingVload VRF5, RF1, $0 //A1LVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //VMAC \\ , VRF3, VRF5, $0010 //VMAC VRF8, VRF4, VRF5, $0111 //All S1HVStore $0 , RF4, VRF8//Then we cycle above 2 steps 8 times till A8H•••// Step4: Reload PsumVload VRF5, RF3, $7 //Parial S8HVMAC VRF8, VRF5, /, $1000 //// Step3: ComputingVload VRF5, RF1, $7 //A8LVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //VMAC \\ , VRF3, VRF5, $0010 //VMAC VRF8, VRF4, VRF5, $0111 //All S8HVStore $7 , RF4, VRF8// Step1:exchange the weight B and Psum SVload VRF1, RF2, $13 //B5LVload VRF2, RF2, $14 //B6LVload VRF3, RF2, $15 //B7LVload VRF4, RF2, $16 //B8L// Step4: Reload PsumVload VRF5, RF4, $8 //Parial S1LVMAC VRF8, VRF5, /, $1000 //// Step3: ComputingVload VRF5, RF1, $8 //A1LVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //VMAC \\ , VRF3, VRF5, $0010 //VMAC VRF8, VRF4, VRF5, $0111 //All S1LVStore $8 , RF4, VRF8//Then we cycle above 2 steps 8 times till A8H•••// Step4: Reload PsumVload VRF5, RF3, $15 //Parial S8LVMAC VRF8, VRF5, /, $1000 //// Step3: ComputingVload VRF5, RF1, $15 //A8LVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //VMAC \\ , VRF3, VRF5, $0010 //VMAC VRF8, VRF4, VRF5, $0111 //All S8LVStore $15 , RF4, VRF8 It’s worthy adding that the assembly code include 4 big cycles and 8 small cycles , in actual situation, we can replace them by callq , jz and loop to simply the code. Besides, we can calculate that,number of DCM fetches is 112:Matrix A: 8 * 4 loadMatrix B: 4 * 4 loadMatrix C: 8 * 2 loadMatrix S: 8 * 2 * 2 store 8 * 2 load And this block matrix can be expand with the increasing of matrices’ size. Solution(2)As described in the above method, we have to reload each elements of matrix A from DCM vector twice. Besides, the matrix sum also need to reload once and store twice to complete twice MAC. So I change the split matrices methods, and in this method, the matrix sum only need store once. The 48 input vectors are divided as follow: $$ \\left[ \\begin{matrix} A1H &amp; A1L \\\\ A2H &amp; A2L \\\\ A3H &amp; A3L \\\\ A4H &amp; A4L \\\\ A5H &amp; A5L \\\\ A6H &amp; A6L \\\\ A7H &amp; A7L \\\\ A8H &amp; A8L \\end{matrix} \\right] * \\left[ \\begin{matrix} B1H &amp; B2H &amp; B3H &amp; B4H &amp; B5H &amp; B6H &amp; B7H &amp; B8H\\\\ B1L &amp; B2L &amp; B3L &amp; B4L &amp; B5L &amp; B6L &amp; B7L &amp; B8L \\end{matrix} \\right] +$$$$ \\left[ \\begin{matrix} C1H &amp; C2H &amp; C3H &amp; C4H\\\\ C5H &amp; C6H &amp; C7H &amp; C8H\\\\ C1L &amp; C2L &amp; C3L &amp; C4L\\\\ C5L &amp; C6L &amp; C7L &amp; C8L \\end{matrix} \\right] = \\left[ \\begin{matrix} S1H &amp; S2H &amp; S3H &amp; S4H\\\\ S5H &amp; S6H &amp; S7H &amp; S8H\\\\ S1L &amp; S2L &amp; S3L &amp; S4L\\\\ S5L &amp; S6L &amp; S7L &amp; S8L \\end{matrix} \\right] $$ I still hold on the weight stationary,so the matrix B is divided in four parts.But this way I split the weight matrix B into 4 parts spanning multiple lines like this:$$\\left[ \\begin{matrix} B1H &amp; B2H\\\\ B1L &amp; B2L \\end{matrix} \\right]\\left[ \\begin{matrix} B3H &amp; B4H \\\\ B3L &amp; B4L \\end{matrix}\\right]\\left[ \\begin{matrix} B5H &amp; B6H \\\\ B5L &amp; B6L \\end{matrix} \\right]\\left[ \\begin{matrix} B7H &amp; B8H \\\\ B7L &amp; B8L \\end{matrix}\\right]$$So the cycle calculation is a little different from the method a.In a word,my design assembly code looks like this : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152Vload VRF6, RF1, $1 //load A1LVMAC \\ , VRF3, VRF6, $0000 //VMAC \\ , VRF4, VRF6, $0001 //Partial S1H=A1H * BiH + A1L * BiL + C1HVload VRF5, RF1, $0 //load A2HVMAC \\ , VRF1, VRF5, $0010 //VMAC \\ , VRF2, VRF5, $0011 //Vload VRF6, RF1, $1 //load A2LVMAC \\ , VRF3, VRF6, $0010 //VMAC VRF8, VRF4, VRF6, $0111 //All S1H=AiH * BiH + AiL * BiL + C1HVStore $0 , RF4, VRF8 //store S1H//Then we cycle above 2 steps 4 times till A8H•••//Change the matrix weight B other 3 splits//Repeat above 4 small cycles•••// Step1:load the weight// Matric B split1Vload VRF1, RF2, $0 //load B7HVload VRF2, RF2, $1 //load B8HVload VRF3, RF2, $2 //load B7LVload VRF4, RF2, $3 //load B8L// Step2: Initial BiasVload VRF5, RF3, $0 //load C4HVMAC VRF8, VRF5, /, $1000 //Add C4H// Step3: ComputingVload VRF5, RF1, $0 //load A1HVMAC \\ , VRF1, VRF5, $0000 //VMAC \\ , VRF2, VRF5, $0001 //Vload VRF6, RF1, $1 //load A1LVMAC \\ , VRF3, VRF6, $0000 //VMAC \\ , VRF4, VRF6, $0001 //Partial S1H=A1H * BiH + A1L * BiL + C1HVload VRF5, RF1, $0 //load A2HVMAC \\ , VRF1, VRF5, $0010 //VMAC \\ , VRF2, VRF5, $0011 //Vload VRF6, RF1, $1 //load A2LVMAC \\ , VRF3, VRF6, $0010 //VMAC VRF8, VRF4, VRF6, $0111 //All S1H=AiH * BiH + AiL * BiL + C1HVStore $0 , RF4, VRF8 //store S1H//Then we cycle above 2 steps 4 times till A8H••• Although the matrix sum only store once, in this method, we have to load matrix A fourth to complete MAC. So we can calculate that,number of DCM fetches is 112,too:Matrix A: 16 * 4 loadMatrix B: 4 * 4 loadMatrix C: 4 * 4 loadMatrix S: 4 * 4 storeOf coarse, there are many other functions such as based on actvation stationary or output stationary. Whatever,choose the reasonable splited methods to balance memory wall and latency depends your own situation. Problem(b)Based on function 1,we proposed a superscalar strategy. As I said above, this method include 4 big cycles and 8 small cycles, so I just analyze the non-repeated instruction parallelism. Assumed that ,all steps cost the same time. As Figure 1,in customed pipeline,we have 4 big cycles and each big cycle has 8 small cycles.And we can figure that,the small cycles’ throughout is 11 cycles.Combined above, we can calculate the throughout of whole assembly code: $$ ((8 * (11 - 2) + 2 + 4) - 2) * 4 + 2 = 306 clks $$Before superscalar strategy, we assumed that, the DCM vector is TrueDualPortRam. Or we can design two DCM to store for weight and activation data seperatly. In other words, we can load or store two vectors based on different address at the same time. And Based on Lecture 4, we use the Load+Mac instruction. In other words, we proposed a LoMac instruction to complete two function in 5 cycles.But it must execute with a load instruction at the same time. 12Vload Vrd(3b), rs1(5b), imm(5b)VloMac Vrd(3b), rs2(5b), imm(1b), funct(4b) Therefore, most load and LoMac instruction can be run in parallel. But the step2/4(initial bias/reload psum) and MAC in small cycle might cause data hazard. So we have to execute two instructions separately. And the Step1(load weight) can be combined with Step3(Computing).But cause the VloMac only have imm(1b), so We add more base scalar RF to store base address. Specifically, RF1 and RF3 still works for matrix A and C.RF2 is for Psum. Matrix B need RF4 to RF11 to index. The depth of scalar RF is 32, so it’s enough.The improving assembly code show follow: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// Step2: Initial BiasVload VRF5, RF3, $0 //load C1HVLoMac VRF8, \\ , \\, $1000 //Add C1H// Step3: Combine load weight and ComputingVload VRF5, RF1, $0 //load A1HVloMac \\ , RF4, $0, $0000 //load B1H and Mac A1H and B1HVload VRF5, RF1, $0 //load A1HVloMac \\ , RF4, $1, $0001 //load B2H and Mac A1H and B2HVload VRF5, RF1, $0 //load A1HVloMac \\ , RF5, $0, $0010 //load B3H and Mac A1H and B3HVload VRF5, RF1, $0 //load A1HVloMac VRF8, RF5, $1, $0111 //load B4H and Mac A1H and B4HVStore $0 , RF2, VRF8 //store partial S1H//Then we cycle above step 8 times till A8H•••// Step2: Initial BiasVload VRF5, RF3, $7 //load C8HVLoMac VRF8, \\ , \\, $1000 //Add C8H// Step3: Combine load weight and ComputingVload VRF5, RF1, $1 //load A8HVloMac \\ , RF4, $0, $0000 //load B1H and Mac A8H and B1HVload VRF5, RF1, $0 //load A1HVloMac \\ , RF4, $1, $0001 //load B2H and Mac A8H and B2HVload VRF5, RF1, $0 //load A1HVloMac \\ , RF5, $0, $0010 //load B3H and Mac A8H and B3HVload VRF5, RF1, $0 //load A1HVloMac VRF8, RF5, $1, $0111 //load B4H and Mac A8H and B4HVStore $7 , RF2, VRF8 //store partial S8H//Step2: exchange the weight B, Bias C and Psum SVload VRF5, RF3, $8 //load C1LVLoMac VRF8, \\ , \\, $1000 //Add C1L// Step3: Combine load weight and ComputingVload VRF5, RF1, $0 //load A1HVloMac \\ , RF6, $0, $0000 //load B1H and Mac A1H and B1HVload VRF5, RF1, $0 //load A1HVloMac \\ , RF6, $1, $0001 //load B2H and Mac A1H and B2HVload VRF5, RF1, $0 //load A1HVloMac \\ , RF7, $0, $0010 //load B3H and Mac A1H and B3HVload VRF5, RF1, $0 //load A1HVloMac VRF8, RF7, $1, $0111 //load B4H and Mac A1H and B4HVStore $0 , RF2, VRF8 //store partial S1L//Then we cycle above step 8 times till A8H•••// Step2: Initial BiasVload VRF5, RF3, $15 //load C8LVLoMac VRF8, \\ , \\, $1000 //Add C8L// Step3: Combine load weight and ComputingVload VRF5, RF1, $1 //load A8HVloMac \\ , RF6, $0, $0000 //load B1H and Mac A8H and B1HVload VRF5, RF1, $0 //load A1HVloMac \\ , RF6, $1, $0001 //load B2H and Mac A8H and B2HVload VRF5, RF1, $0 //load A1HVloMac \\ , RF7, $0, $0010 //load B3H and Mac A8H and B3HVload VRF5, RF1, $0 //load A1HVloMac VRF8, RF7, $1, $0111 //load B4H and Mac A8H and B4HVStore $7 , RF2, VRF8 //store partial S8L•••//The same for 4 cycles, except that, we don't need matrix C to the Psum,//But we reload the calculated Psum as the initial Bias.//Refer to the assembly code in Page 4 for details. As Figure 2,we have 4 big cycles and each big cycle only has 8 small cycles with superscalar strategy.And we can figure that,the small cycles’ throughout is 9 cycles without VStore. And we have to stall the pipeline 3 cycles for VStore. And this instruction, we cannot realize superscalar.However, just like the yellow rectangle, we can run next cycles’ instruction. And I confirm that if we change the VRF for Psum, we can avoid the data hazard. Combined above, we can calculate the throughout of whole assembly code: $$ (8 * (9 - 4) + 8) * 4 - 1 + 6\\ =\\ 197 clks $$$$Before\\ Superscalar: 5 * 8 * 4 / 306\\ =\\ 52.29%$$$$After\\ Superscalar: 5 * 8 * 4 / 197\\ =\\ 81.22% $$ In this work, we assumed that all step cost the same time. But in reality, ALU-MAC costs much more longer than others, so superscalar strategy could improve the pipeline latency significantly.In this system,we realize a MAC module based on the circuit showed in Problem1.The source code was written by SystemVerilog called mac.sv. And the interface of MAC module was showed follow: 12345678module mac( input logic clk, input logic [31:0]in1, input logic [31:0]in2, input logic [3:0]func, output logic [31:0] out); We also submit the complete source code in the attachment.And the testbench called tb_mac.sv was attached, too. In the testbench, we realize the maxtrix MAC.$$ \\left[ \\begin{matrix} -1 &amp; 2 &amp; -3 &amp; 4 \\\\ \\end{matrix} \\right] * \\left[ \\begin{matrix} -1 &amp; -1 &amp; 0 &amp; 2 \\\\ 2 &amp; 2 &amp; 3 &amp; 1 \\\\ 3 &amp; -3 &amp; 1 &amp; 4 \\\\ 4 &amp; 4 &amp; 2 &amp; 5 \\end{matrix} \\right] + \\left[ \\begin{matrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\end{matrix} \\right] = \\left[ \\begin{matrix} 13 &amp; 32 &amp; 14 &amp; 12 \\\\ \\end{matrix} \\right]$$In Figure3, we realize 5 VMAC instructions to complete the matrix calculation. The final out is a splicing of 4 quantized results in scratchpad of MAC.$0c\\ 0e\\ 20\\ 0d$ is the hexadecimal of result of $12\\ 14\\ 32\\ 13$.We have reached the expected functional goal.The specific source code just like the mac.sv and tb_mac.sv in appendix.I also attached a screenshot of the simulation waveform. ******Believe me, Support me, Pay me!******⬇︎","link":"/2021/04/30/CA-HW-2/"},{"title":"Zhutmost and his little cute","text":"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 Welcome to Secrect of Little cute, enter password to read.(Hint:Anniversary)","link":"/2021/05/19/Zhutmost-and-his-little-cute/"}],"tags":[],"categories":[{"name":"KWS","slug":"KWS","link":"/categories/KWS/"},{"name":"Computer and Intelligent Processor Architecture","slug":"Computer-and-Intelligent-Processor-Architecture","link":"/categories/Computer-and-Intelligent-Processor-Architecture/"}]}